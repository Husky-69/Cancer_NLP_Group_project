{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Chatbot for Cancer Information Retrieval\n",
    "\n",
    "This notebook serves as the primary document for the **Cancer Information Retrieval NLP Model**, a retrieval-based Natural Language Processing (NLP) system designed to provide accurate and immediate answers to user queries related to cancer. This project aims to enhance access to trustworthy medical information, alleviate the burden of routine inquiries on healthcare staff, and combat the spread of misinformation often found online.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business Understanding\n",
    "\n",
    "This section introduces the real-world problem this project aims to solve, outlines the specific project objectives, and identifies the key stakeholders who would benefit from this solution. \n",
    "\n",
    "### 1.1 Problem Statement\n",
    "\n",
    "The increasing volume of online health information, coupled with the high demand for medical insights and the limited capacity of healthcare professionals, creates a significant challenge. Patients often struggle to find reliable, cancer-specific information quickly, leading to misinformation and increased strain on healthcare support systems.\n",
    "\n",
    "### 1.2 Project Objectives\n",
    "\n",
    "The key objectives of this Cancer Information Retrieval NLP Model project are to:\n",
    "\n",
    "* **Develop a robust text preprocessing pipeline:** Clean and normalize medical text data from the MedQuAD dataset to ensure optimal input for NLP models.\n",
    "* **Implement an effective retrieval mechanism:** Design and build a system (e.g., using TF-IDF or semantic embeddings) capable of accurately matching user queries to relevant questions/answers in the dataset.\n",
    "* **Evaluate model performance:** Assess the retrieval model's accuracy and efficiency using appropriate metrics and test cases to ensure it meets the project's goals.\n",
    "* **Prepare for API integration:** Structure the core NLP model's components to be easily deployable as a backend service for potential chatbot applications.\n",
    "* **Enhance user experience:** Contribute to a system that provides fast, reliable, and relevant information, ultimately improving how users access cancer-related knowledge.\n",
    "\n",
    "### 1.3 Stakeholders\n",
    "\n",
    "The primary stakeholders for this project are:\n",
    "\n",
    "* **Patients/General Public:** Who need quick, reliable access to cancer information without sifting through unreliable online sources.\n",
    "* **Healthcare Providers/Hospitals:** Who can integrate this bot backend to streamline their patient support services and reduce manual inquiry handling.\n",
    "* **Medical Researchers/Educators:** Who can use the structured Q&A data for analysis or educational purposes.\n",
    "\n",
    "This NLP model is designed to serve as a robust backend for integration into various bot platforms, optimizing information dissemination and combating misinformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Library Imports\n",
    "\n",
    "This section handles the necessary library imports and initial setup required for data processing and model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import warnings\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 NLTK Downloads\n",
    "\n",
    "Some NLTK functionalities require specific datasets to be downloaded. The following cell ensures these are available for text processing tasks like tokenization and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\biggm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\biggm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\biggm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\biggm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download improved NLP resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Data Understanding\n",
    "\n",
    "### 3.1 Data Loading and Initial Overview\n",
    "\n",
    "The core of this project relies on the **MedQuAD: Medical Question-Answer Dataset**, which is available on Kaggle. This dataset is a valuable resource containing medical questions and their corresponding expert answers, highly suitable for building a retrieval-based information system.\n",
    "\n",
    "#### 3.1.1 Custom Data Loader: `CancerQALoader`\n",
    "\n",
    "To efficiently process the XML structured data from the MedQuAD dataset, a custom Python class `CancerQALoader` has been developed. This class is responsible for:\n",
    "* Parsing the XML files to extract relevant question-answer pairs.\n",
    "* Handling potential parsing errors.\n",
    "* Consolidating data from multiple XML files within a specified folder into a single, clean Pandas DataFrame.\n",
    "\n",
    "Below is the definition of the `CancerQALoader` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CancerQALoader:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.questions = []\n",
    "        self.answers = []\n",
    "        self.root = None\n",
    "        self.source = os.path.splitext(os.path.basename(filepath))[0]\n",
    "\n",
    "    def parse_xml(self):\n",
    "        try:\n",
    "            tree = ET.parse(self.filepath)\n",
    "            self.root = tree.getroot()\n",
    "        except ET.ParseError as e:\n",
    "            print(f\"Error parsing XML in {self.filepath}: {e}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {self.filepath}\")\n",
    "\n",
    "    def extract_qa_pairs(self):\n",
    "        if self.root is None:\n",
    "            return\n",
    "\n",
    "        for qa_pair in self.root.findall('.//QAPair'):\n",
    "            question_element = qa_pair.find('Question')\n",
    "            answer_element = qa_pair.find('Answer')\n",
    "\n",
    "            question = question_element.text if question_element is not None else None\n",
    "            answer = answer_element.text if answer_element is not None else None\n",
    "\n",
    "            if question and answer:\n",
    "                self.questions.append(question)\n",
    "                self.answers.append(answer)\n",
    "\n",
    "    def get_dataframe(self):\n",
    "        if not self.questions or not self.answers:\n",
    "            return pd.DataFrame(columns=['question', 'answer', 'source'])\n",
    "\n",
    "        return pd.DataFrame({\n",
    "            'question': self.questions,\n",
    "            'answer': self.answers,\n",
    "            'source': [self.source] * len(self.questions)\n",
    "        })\n",
    "\n",
    "    def load_all_qa_from_folder(self, folder_path):\n",
    "        all_dfs = []\n",
    "        if not os.path.isdir(folder_path):\n",
    "            print(f\"Error: Folder not found: {folder_path}\")\n",
    "            return pd.DataFrame(columns=['question', 'answer', 'source'])\n",
    "\n",
    "        xml_files = [f for f in os.listdir(folder_path) if f.endswith(\".xml\")]\n",
    "        if not xml_files:\n",
    "            print(f\"Warning: No XML files found in {folder_path}. Please check the folder content.\")\n",
    "            return pd.DataFrame(columns=['question', 'answer', 'source'])\n",
    "\n",
    "        for filename in xml_files:\n",
    "            full_path = os.path.join(folder_path, filename)\n",
    "            loader = CancerQALoader(full_path)\n",
    "            loader.parse_xml()\n",
    "            loader.extract_qa_pairs()\n",
    "            df = loader.get_dataframe()\n",
    "            if not df.empty:\n",
    "                all_dfs.append(df)\n",
    "\n",
    "        if not all_dfs:\n",
    "            print(f\"Warning: XML files found in '{folder_path}' but no Q&A pairs were successfully extracted from any of them. Check XML structure and XPath.\")\n",
    "\n",
    "        return pd.concat(all_dfs, ignore_index=True) if all_dfs else pd.DataFrame(columns=['question', 'answer', 'source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully from '../data/1_CancerGov_QA'.\n",
      "Number of unique source files loaded: 116 files.\n"
     ]
    }
   ],
   "source": [
    "folder = \"../data/1_CancerGov_QA\"\n",
    "\n",
    "data_loader = CancerQALoader(filepath=\"dummy_path.xml\")\n",
    "\n",
    "try:\n",
    "    cancer_df = data_loader.load_all_qa_from_folder(folder)\n",
    "    \n",
    "    print(f\"Dataset loaded successfully from '{folder}'.\")\n",
    "    print(f\"Number of unique source files loaded: {cancer_df['source'].nunique()} files.\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The folder '{folder}' was not found. Please verify the path to your MedQuAD XML files based on your project structure.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during data loading: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Dataset Suitability and Feature Description\n",
    "\n",
    "The MedQuAD dataset, with its structured question-answer pairs extracted from trusted medical sources (like CancerGov.com in this case), is exceptionally well-suited for building a retrieval-based NLP model. Each entry directly provides a query (`question`) and its authoritative response (`answer`), which perfectly aligns with the input-output requirements of a conversational AI backend.\n",
    "\n",
    "The primary features (columns) extracted and used from the MedQuAD dataset are:\n",
    "\n",
    "* **`question`**: This column contains the original medical questions. These will serve as the knowledge base against which incoming user queries are compared to find the most relevant match. The inclusion of this feature is justified as it directly provides the query component necessary for a Q&A system.\n",
    "* **`answer`**: This column contains the detailed, expert-provided answers corresponding to each question. This is the information that will be retrieved and delivered to the user as the system's response. Its inclusion is justified as it provides the authoritative response for the system.\n",
    "* **`source`**: This column indicates the original medical website (e.g., CancerGov.com) from which the Q&A pair was extracted. Its inclusion is justified as it provides valuable metadata, ensures transparency, and reinforces the trustworthiness of the information provided, which is critical in a medical context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Displaying the first 20 rows of the DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is (are) Adult Acute Lymphoblastic Leukem...</td>\n",
       "      <td>Key Points\\n                    - Adult acute ...</td>\n",
       "      <td>0000001_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the symptoms of Adult Acute Lymphobla...</td>\n",
       "      <td>Signs and symptoms of adult ALL include fever,...</td>\n",
       "      <td>0000001_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to diagnose Adult Acute Lymphoblastic Leuk...</td>\n",
       "      <td>Tests that examine the blood and bone marrow a...</td>\n",
       "      <td>0000001_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the outlook for Adult Acute Lymphoblas...</td>\n",
       "      <td>Certain factors affect prognosis (chance of re...</td>\n",
       "      <td>0000001_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who is at risk for Adult Acute Lymphoblastic L...</td>\n",
       "      <td>Previous chemotherapy and exposure to radiatio...</td>\n",
       "      <td>0000001_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What are the stages of Adult Acute Lymphoblast...</td>\n",
       "      <td>Key Points\\n                    - Once adult A...</td>\n",
       "      <td>0000001_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are the treatments for Adult Acute Lympho...</td>\n",
       "      <td>Key Points\\n                    - There are di...</td>\n",
       "      <td>0000001_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is (are) Adult Acute Myeloid Leukemia ?</td>\n",
       "      <td>Key Points\\n                    - Adult acute ...</td>\n",
       "      <td>0000001_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Who is at risk for Adult Acute Myeloid Leukemi...</td>\n",
       "      <td>Smoking, previous chemotherapy treatment, and ...</td>\n",
       "      <td>0000001_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What are the symptoms of Adult Acute Myeloid L...</td>\n",
       "      <td>Signs and symptoms of adult AML include fever,...</td>\n",
       "      <td>0000001_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How to diagnose Adult Acute Myeloid Leukemia ?</td>\n",
       "      <td>Tests that examine the blood and bone marrow a...</td>\n",
       "      <td>0000001_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What is the outlook for Adult Acute Myeloid Le...</td>\n",
       "      <td>Certain factors affect prognosis (chance of re...</td>\n",
       "      <td>0000001_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What are the stages of Adult Acute Myeloid Leu...</td>\n",
       "      <td>Key Points\\n                    - Once adult a...</td>\n",
       "      <td>0000001_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What are the treatments for Adult Acute Myeloi...</td>\n",
       "      <td>Key Points\\n                    - There are di...</td>\n",
       "      <td>0000001_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>what research (or clinical trials) is being do...</td>\n",
       "      <td>New types of treatment are being tested in cli...</td>\n",
       "      <td>0000001_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What is (are) Chronic Lymphocytic Leukemia ?</td>\n",
       "      <td>Key Points\\n                    - Chronic lymp...</td>\n",
       "      <td>0000001_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Who is at risk for Chronic Lymphocytic Leukemi...</td>\n",
       "      <td>Older age can affect the risk of developing ch...</td>\n",
       "      <td>0000001_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What are the symptoms of Chronic Lymphocytic L...</td>\n",
       "      <td>Signs and symptoms of chronic lymphocytic leuk...</td>\n",
       "      <td>0000001_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How to diagnose Chronic Lymphocytic Leukemia ?</td>\n",
       "      <td>Tests that examine the blood, bone marrow, and...</td>\n",
       "      <td>0000001_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What is the outlook for Chronic Lymphocytic Le...</td>\n",
       "      <td>Certain factors affect treatment options and p...</td>\n",
       "      <td>0000001_3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   What is (are) Adult Acute Lymphoblastic Leukem...   \n",
       "1   What are the symptoms of Adult Acute Lymphobla...   \n",
       "2   How to diagnose Adult Acute Lymphoblastic Leuk...   \n",
       "3   What is the outlook for Adult Acute Lymphoblas...   \n",
       "4   Who is at risk for Adult Acute Lymphoblastic L...   \n",
       "5   What are the stages of Adult Acute Lymphoblast...   \n",
       "6   What are the treatments for Adult Acute Lympho...   \n",
       "7        What is (are) Adult Acute Myeloid Leukemia ?   \n",
       "8   Who is at risk for Adult Acute Myeloid Leukemi...   \n",
       "9   What are the symptoms of Adult Acute Myeloid L...   \n",
       "10     How to diagnose Adult Acute Myeloid Leukemia ?   \n",
       "11  What is the outlook for Adult Acute Myeloid Le...   \n",
       "12  What are the stages of Adult Acute Myeloid Leu...   \n",
       "13  What are the treatments for Adult Acute Myeloi...   \n",
       "14  what research (or clinical trials) is being do...   \n",
       "15       What is (are) Chronic Lymphocytic Leukemia ?   \n",
       "16  Who is at risk for Chronic Lymphocytic Leukemi...   \n",
       "17  What are the symptoms of Chronic Lymphocytic L...   \n",
       "18     How to diagnose Chronic Lymphocytic Leukemia ?   \n",
       "19  What is the outlook for Chronic Lymphocytic Le...   \n",
       "\n",
       "                                               answer     source  \n",
       "0   Key Points\\n                    - Adult acute ...  0000001_1  \n",
       "1   Signs and symptoms of adult ALL include fever,...  0000001_1  \n",
       "2   Tests that examine the blood and bone marrow a...  0000001_1  \n",
       "3   Certain factors affect prognosis (chance of re...  0000001_1  \n",
       "4   Previous chemotherapy and exposure to radiatio...  0000001_1  \n",
       "5   Key Points\\n                    - Once adult A...  0000001_1  \n",
       "6   Key Points\\n                    - There are di...  0000001_1  \n",
       "7   Key Points\\n                    - Adult acute ...  0000001_2  \n",
       "8   Smoking, previous chemotherapy treatment, and ...  0000001_2  \n",
       "9   Signs and symptoms of adult AML include fever,...  0000001_2  \n",
       "10  Tests that examine the blood and bone marrow a...  0000001_2  \n",
       "11  Certain factors affect prognosis (chance of re...  0000001_2  \n",
       "12  Key Points\\n                    - Once adult a...  0000001_2  \n",
       "13  Key Points\\n                    - There are di...  0000001_2  \n",
       "14  New types of treatment are being tested in cli...  0000001_2  \n",
       "15  Key Points\\n                    - Chronic lymp...  0000001_3  \n",
       "16  Older age can affect the risk of developing ch...  0000001_3  \n",
       "17  Signs and symptoms of chronic lymphocytic leuk...  0000001_3  \n",
       "18  Tests that examine the blood, bone marrow, and...  0000001_3  \n",
       "19  Certain factors affect treatment options and p...  0000001_3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nDisplaying the first 20 rows of the DataFrame:\")\n",
    "display(cancer_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset shape: 729 rows, 3 columns\n"
     ]
    }
   ],
   "source": [
    "#Checking the shape of the DataFrame to see how many rows and columns it contains\n",
    "print(f\"\\nDataset shape: {cancer_df.shape[0]} rows, {cancer_df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 729 entries, 0 to 728\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   question  729 non-null    object\n",
      " 1   answer    729 non-null    object\n",
      " 2   source    729 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 17.2+ KB\n"
     ]
    }
   ],
   "source": [
    "#Checking the information about the DataFrame, including data types and non-null counts\n",
    "print(\"\\nDataset Information:\")\n",
    "cancer_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Descriptive Statistics and Data Limitations\n",
    "\n",
    "A crucial step in data understanding is to examine the dataset's descriptive statistics and identify any inherent limitations. This helps in making informed decisions for subsequent data preparation and modeling.\n",
    "\n",
    "The dataset consists of `question`, `answer`, and `source` columns. As these are primarily text-based features, traditional numerical descriptive statistics (like mean, std dev) are not directly applicable. Instead, we focus on properties relevant to text data:\n",
    "\n",
    "* **Dataset Size:** As observed during loading, the dataset contains 729 entries (Q&A pairs) and 3 columns.\n",
    "* **Unique Sources:** There are 116 distinct source files from which the Q&A pairs were extracted, indicating the breadth of the medical topics covered.\n",
    "* **Null Values:** We will explicitly check for any missing values in the `question` or `answer` columns, as these would directly impact the model's performance.\n",
    "\n",
    "**Data Limitations:**\n",
    "While comprehensive, this dataset might have certain limitations that could impact the project:\n",
    "\n",
    "* **Specificity of Questions:** The questions are related to \"cancer.\" The model's effectiveness will be limited to this domain; it cannot answer general medical queries.\n",
    "* **Vocabulary and Jargon:** Medical text often contains highly specialized vocabulary. While necessary for accuracy, this might require robust text preprocessing (e.g., handling acronyms, medical terms not in standard dictionaries) and potentially large vocabulary models.\n",
    "* **Ambiguity and Nuance:** Natural language, especially in a medical context, can be ambiguous or require nuanced understanding (e.g., distinguishing similar conditions). A retrieval-based model may struggle with highly nuanced or subjective queries.\n",
    "* **Static Dataset:** As a static dataset, it won't inherently update with new medical discoveries or evolving information. A production system would require mechanisms for periodic data refreshes.\n",
    "* **Bias in Source Data:** Although from reputable sources, any inherent biases or outdated information present in the original XML files will be reflected in the model's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive Statistics for the Dataset:\n",
      "Missing values per column:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "question    0\n",
       "answer      0\n",
       "source      0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of Q&A pairs: 729\n",
      "Number of unique questions: 683\n",
      "Number of unique answers: 705\n",
      "Number of unique sources: 116\n",
      "\n",
      "Example Question-Answer Pairs (5 random samples):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>What is the outlook for Metastatic Squamous Ne...</td>\n",
       "      <td>Certain factors affect prognosis (chance of re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>How to prevent Anal Cancer ?</td>\n",
       "      <td>Key Points\\n                    - Avoiding ris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>What are the stages of Myelodysplastic/ Myelop...</td>\n",
       "      <td>Key Points\\n                    - There is no ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>What is the outlook for Extragonadal Germ Cell...</td>\n",
       "      <td>Certain factors affect prognosis (chance of re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>What is (are) Pancreatic Neuroendocrine Tumors...</td>\n",
       "      <td>Key Points\\n                    - Pancreatic n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "468  What is the outlook for Metastatic Squamous Ne...   \n",
       "148                       How to prevent Anal Cancer ?   \n",
       "302  What are the stages of Myelodysplastic/ Myelop...   \n",
       "355  What is the outlook for Extragonadal Germ Cell...   \n",
       "515  What is (are) Pancreatic Neuroendocrine Tumors...   \n",
       "\n",
       "                                                answer  \n",
       "468  Certain factors affect prognosis (chance of re...  \n",
       "148  Key Points\\n                    - Avoiding ris...  \n",
       "302  Key Points\\n                    - There is no ...  \n",
       "355  Certain factors affect prognosis (chance of re...  \n",
       "515  Key Points\\n                    - Pancreatic n...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Descriptive Statistics for the Dataset:\")\n",
    "\n",
    "print(\"Missing values per column:\")\n",
    "display(cancer_df.isnull().sum())\n",
    "\n",
    "print(f\"\\nTotal number of Q&A pairs: {cancer_df.shape[0]}\")\n",
    "print(f\"Number of unique questions: {cancer_df['question'].nunique()}\")\n",
    "print(f\"Number of unique answers: {cancer_df['answer'].nunique()}\")\n",
    "print(f\"Number of unique sources: {cancer_df['source'].nunique()}\")\n",
    "\n",
    "print(\"\\nExample Question-Answer Pairs (5 random samples):\")\n",
    "display(cancer_df[['question', 'answer']].sample(5, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning & Preprocessing\n",
    "\n",
    "This section is dedicated to transforming the raw MedQuAD dataset into a clean and structured format suitable for building a robust NLP model. It involves two primary phases: initial data cleaning (addressing issues like duplicate entries) and detailed text preprocessing (normalizing and preparing the textual content of questions and answers).\n",
    "\n",
    "### 4.1 Initial Data Cleaning: Handling Duplicates\n",
    "\n",
    "The first step in ensuring data quality is to identify and address redundant entries. In this dataset, it's crucial to ensure that we don't have multiple identical questions, as this could skew our model's understanding or lead to redundant retrievals. We will check for duplicate questions and remove them, retaining the first occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "729"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the number of rows in the DataFrame to see how many Q&A pairs were loaded\n",
    "len(cancer_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the data type of the DataFrame to confirm it is a pandas DataFrame\n",
    "type(cancer_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicate questions: 78\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>What is (are) Chronic Myelogenous Leukemia ?</td>\n",
       "      <td>Key Points\\n                    - Chronic myel...</td>\n",
       "      <td>0000001_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>What are the treatments for Chronic Myelogenou...</td>\n",
       "      <td>Key Points\\n                    - There are di...</td>\n",
       "      <td>0000001_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>What is (are) Liver (Hepatocellular) Cancer ?</td>\n",
       "      <td>Key Points\\n                    - Liver cancer...</td>\n",
       "      <td>0000007_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Who is at risk for Liver (Hepatocellular) Canc...</td>\n",
       "      <td>Being infected with certain types of the hepat...</td>\n",
       "      <td>0000007_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>What is (are) Liver (Hepatocellular) Cancer ?</td>\n",
       "      <td>Key Points\\n                    - Liver cancer...</td>\n",
       "      <td>0000007_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>Who is at risk for Prostate Cancer? ?</td>\n",
       "      <td>Different factors increase or decrease the ris...</td>\n",
       "      <td>0000036_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>What is (are) Colorectal Cancer ?</td>\n",
       "      <td>Key Points\\n                    - Colorectal c...</td>\n",
       "      <td>0000037_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>Who is at risk for Colorectal Cancer? ?</td>\n",
       "      <td>Key Points\\n                    - Avoiding ris...</td>\n",
       "      <td>0000037_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>What is (are) Colorectal Cancer ?</td>\n",
       "      <td>Key Points\\n                    - Colorectal c...</td>\n",
       "      <td>0000037_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>Who is at risk for Colorectal Cancer? ?</td>\n",
       "      <td>Different factors increase or decrease the ris...</td>\n",
       "      <td>0000037_4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "22        What is (are) Chronic Myelogenous Leukemia ?   \n",
       "29   What are the treatments for Chronic Myelogenou...   \n",
       "246      What is (are) Liver (Hepatocellular) Cancer ?   \n",
       "247  Who is at risk for Liver (Hepatocellular) Canc...   \n",
       "250      What is (are) Liver (Hepatocellular) Cancer ?   \n",
       "..                                                 ...   \n",
       "678              Who is at risk for Prostate Cancer? ?   \n",
       "695                  What is (are) Colorectal Cancer ?   \n",
       "697            Who is at risk for Colorectal Cancer? ?   \n",
       "699                  What is (are) Colorectal Cancer ?   \n",
       "700            Who is at risk for Colorectal Cancer? ?   \n",
       "\n",
       "                                                answer     source  \n",
       "22   Key Points\\n                    - Chronic myel...  0000001_4  \n",
       "29   Key Points\\n                    - There are di...  0000001_4  \n",
       "246  Key Points\\n                    - Liver cancer...  0000007_4  \n",
       "247  Being infected with certain types of the hepat...  0000007_4  \n",
       "250  Key Points\\n                    - Liver cancer...  0000007_5  \n",
       "..                                                 ...        ...  \n",
       "678  Different factors increase or decrease the ris...  0000036_3  \n",
       "695  Key Points\\n                    - Colorectal c...  0000037_3  \n",
       "697  Key Points\\n                    - Avoiding ris...  0000037_3  \n",
       "699  Key Points\\n                    - Colorectal c...  0000037_4  \n",
       "700  Different factors increase or decrease the ris...  0000037_4  \n",
       "\n",
       "[78 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many duplicate questions exist\n",
    "duplicate_questions = cancer_df[cancer_df.duplicated(subset='question', keep=False)]\n",
    "print(f\"Total duplicate questions: {duplicate_questions.shape[0]}\")\n",
    "duplicate_questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing duplicate questions from the DataFrame while keeping the first occurrence\n",
    "cancer_df=cancer_df.drop_duplicates(subset='question', keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the shape of the DataFrame after removing duplicates to see how many rows remain\n",
    "cancer_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Text Preprocessing: Normalizing Questions and Answers\n",
    "\n",
    "After initial cleaning, the textual content of the question and answer columns requires meticulous preprocessing to enhance consistency and relevance for NLP tasks. Medical text often presents unique challenges, such as specialized terminology, boilerplate phrases, and variations in formatting. The following steps standardize the text, making it more digestible for the retrieval model.\n",
    "\n",
    "#### Defining a Custom Text Cleaning Function (clean_text)\n",
    "To handle the specific characteristics of medical text, a custom function clean_text is implemented. This function performs a series of transformations:\n",
    "\n",
    "* Medical-Specific Stopwords: A curated set of stopwords (MEDICAL_STOPWORDS) is defined. These are terms that, while common in English, might not add significant value in the context of specific medical inquiries (e.g., 'patient', 'doctor').\n",
    "\n",
    "* Boilerplate and Special Character Removal: Regular expressions are used to eliminate common boilerplate phrases (like \"Key Points:*-\"), standardize newline characters and multiple spaces, and remove any remaining HTML tags.\n",
    "\n",
    "* Case Normalization and Hyphen Preservation: All text is converted to lowercase to ensure uniformity. Importantly, hyphens within medical terms (e.g., \"T-cell\") are preserved to maintain the integrity and meaning of these specialized phrases.\n",
    "\n",
    "* Tokenization: The text is broken down into individual words or subword units (tokens) using word_tokenize.\n",
    "\n",
    "* Filtering and Medical Plural Handling:Tokens shorter than two characters are generally removed, with exceptions for medical stopwords that might be short but relevant.\n",
    "\n",
    "* A specialized rule is applied to standardize common medical plural forms (e.g., 'therapies' to 'therapy', 'diseases' to 'disease') to ensure consistency before lemmatization.\n",
    "\n",
    "* Lemmatization: The WordNetLemmatizer reduces words to their base or dictionary form (e.g., 'running' becomes 'run'), further standardizing the vocabulary.\n",
    "\n",
    "This multi-stage cleaning approach ensures that the dataset's textual content is refined, removing noise while retaining critical medical context, which is vital for the accuracy of our retrieval system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New temporary cell to inspect data cleaning impact\n",
    "print(\"Original Answer Example:\")\n",
    "# Get a random row to inspect, preferably one that was giving jibberish\n",
    "original_answer_example = cancer_df.iloc[0]['answer'] # Or pick a specific index\n",
    "print(original_answer_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to clean and preprocess the text data in the DataFrame\n",
    "MEDICAL_STOPWORDS = set(stopwords.words('english')) | {\n",
    "    'patient', 'cancer', 'may', 'also', 'include', 'following',\n",
    "    'key', 'point', 'points', 'doctor', 'medical'\n",
    "}\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Improved medical text cleaning with boilerplate removal and specialized handling.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # 1. Remove all boilerplate patterns, newlines, and collapse multiple spaces\n",
    "    text = re.sub(r'Key Points.*?-', '', text)\n",
    "    text = re.sub(r'\\\\n|\\n', ' ', text) # Replace newlines with spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)    # Collapse multiple spaces\n",
    "\n",
    "    # 2. Remove HTML tags and keep hyphens in medical terms, then convert to lowercase\n",
    "    text = re.sub(r'<.*?>', '', text)   # Remove HTML tags\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s-]', '', text).lower() # Keep hyphens in terms\n",
    "\n",
    "    # 3. Enhanced medical token handling: tokenization, filtering, medical plural handling, and lemmatization\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = []\n",
    "\n",
    "    for word in tokens:\n",
    "        # Keep medical terms longer than 2 chars, except stopwords\n",
    "        if len(word) > 2 and (word not in MEDICAL_STOPWORDS):\n",
    "            # Special handling for medical plurals (e.g., therapies -> therapy)\n",
    "            lemma = lemmatizer.lemmatize(word, pos='n')\n",
    "            if lemma.endswith('ies'):\n",
    "                lemma = lemma[:-3] + 'y' # e.g., therapies -> therapy\n",
    "            elif lemma.endswith('es'):\n",
    "                lemma = lemma[:-2] # e.g., diseases -> disease (simple case)\n",
    "            filtered_tokens.append(lemma)\n",
    "\n",
    "    return ' '.join(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCleaned Answer Example (after current preprocessing):\")\n",
    "# Assuming clean_text was applied like this:\n",
    "# cancer_df['answer'] = cancer_df['answer'].apply(clean_text)\n",
    "# If it wasn't, you need to explicitly clean it here for testing:\n",
    "cleaned_answer_example = clean_text(original_answer_example)\n",
    "print(cleaned_answer_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Preprocessing to the Dataset\n",
    "The clean_text function is now applied to both the question and answer columns of our cancer_df DataFrame. This ensures that all text data used for subsequent NLP tasks is consistently cleaned and normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove spaces and new lines\n",
    "cancer_df['question'] = cancer_df['question'].str.strip().replace('\\n', ' ')\n",
    "cancer_df['answer_display'] = cancer_df['answer'].str.strip().replace('\\n', ' ')\n",
    "\n",
    "\n",
    "# Preprocess the questions and answers\n",
    "cancer_df['question'] = cancer_df['question'].apply(clean_text)\n",
    "# cancer_df['answer'] = cancer_df['answer'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying Preprocessing Results\n",
    "After applying the preprocessing steps, it's important to verify the changes. We'll re-examine the number of unique answers (which might change slightly due to normalization) and display the first few rows of the DataFrame to visually inspect the cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the number of unique answers in the DataFrame\n",
    "print(f\"Number of unique answers: {cancer_df['answer'].nunique()}\")\n",
    "print(cancer_df['answer'].value_counts().head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the first 10 rows of the DataFrame\n",
    "cancer_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hybrid Search System\n",
    "\n",
    "To build a robust cancer information retrieval system, a hybrid approach leveraging both keyword-based and semantic search capabilities will be implemented. This strategy aims to combine the precision of traditional term matching with the nuanced understanding of modern deep learning models, thereby improving the relevance and comprehensiveness of search results for diverse user queries.\n",
    "\n",
    "### 5.1 Keyword-Based Retrieval: TF-IDF Vectorization\n",
    "\n",
    "Term Frequency-Inverse Document Frequency (TF-IDF) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. It's particularly effective for keyword-based matching and identifying important terms within questions.\n",
    "\n",
    "For our hybrid system, TF-IDF will be used to create sparse vector representations of the `question` column. These vectors will serve as the basis for a keyword-driven retrieval component, allowing us to find questions with significant term overlap with a user's query. The `clean_text` preprocessor is integrated directly to ensure the TF-IDF vectors are generated from normalized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "tfidf_vectorizer = TfidfVectorizer(preprocessor=clean_text, stop_words=list(MEDICAL_STOPWORDS))\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(cancer_df['question'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Semantic Retrieval: BioBERT Question Embeddings\n",
    "Beyond keyword matching, understanding the semantic meaning of questions is crucial for handling rephrased queries or those using synonyms. For this, we utilize a pre-trained BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining) model. BioBERT is a specialized BERT model trained on extensive biomedical text corpora, making it highly effective at generating contextually rich, dense vector embeddings (sentence embeddings) for medical queries.\n",
    "\n",
    "These embeddings capture the nuanced meaning of questions and will enable our system to retrieve answers based on semantic similarity, even if exact keywords are not present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biomedical embedding model\n",
    "model = SentenceTransformer(\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")\n",
    "question_embeddings = model.encode(cancer_df['question'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 2D Visualization of BioBERT Question Embeddings\n",
    "Visualizing high-dimensional embeddings can provide valuable insights into the quality of the representations and reveal underlying structures or clusters within the data. We will use t-Distributed Stochastic Neighbor Embedding (t-SNE) for dimensionality reduction, projecting the BioBERT question embeddings from their high-dimensional space into a 2D plot.\n",
    "\n",
    "The t-SNE plot helps to:\n",
    "\n",
    "* Identify potential clusters of semantically similar questions.\n",
    "\n",
    "* Verify if related questions are grouped together.\n",
    "\n",
    "* Gain an intuitive understanding of the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Visualization of Sentence Embeddings\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(question_embeddings)\n",
    "\n",
    "plt.figure(figsize= (6,5))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.6)\n",
    "plt.title(\"2D Visualization of BioBERT Question Embeddings\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot above displays a **2D t-SNE projection** of semantic embeddings generated by the BioBERT-based SentenceTransformer model for each cancer-related question in the dataset.\n",
    "#### Key Observations:\n",
    "- Several **tight clusters** can be seen, suggesting that BioBERT effectively groups similar medical questions like those about:\n",
    "  - Symptoms and diagnosis\n",
    "  - Cancer types\n",
    "  - Treatment and medications\n",
    "  - Survival and prognosis\n",
    "- **Scattered points** may indicate outliers or uniquely phrased questions that do not fit well into larger semantic groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hybrid Search Function\n",
    "\n",
    "With the text data preprocessed and both TF-IDF and BioBERT semantic embeddings generated, the next critical step is to develop the **hybrid search function**. This function will serve as the core of our chatbot's retrieval mechanism, taking a user's query and returning the most relevant question-answer pairs from our dataset.\n",
    "\n",
    "The `hybrid_search` function combines the strengths of keyword-based matching (using TF-IDF) and semantic understanding (using BioBERT embeddings). It allows for tunable weights to balance the influence of each component and includes a confidence threshold and an optional disease-specific filter for more precise results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query, n=3, tfidf_weight=0.0, semantic_weight=1.0, threshold=0.30, disease_filter=None):\n",
    "    \"\"\"Enhanced hybrid search with disease filtering\"\"\"\n",
    "    # Process query\n",
    "    clean_query = clean_text(query)\n",
    "    \n",
    "    # TF-IDF similarity\n",
    "    tfidf_vec = tfidf_vectorizer.transform([clean_query])\n",
    "    tfidf_sim = cosine_similarity(tfidf_vec, X_tfidf).flatten()\n",
    "    \n",
    "    # Semantic similarity\n",
    "    semantic_vec = model.encode([query])\n",
    "    semantic_sim = cosine_similarity(semantic_vec, question_embeddings).flatten()\n",
    "    \n",
    "    # Combined score\n",
    "    combined_sim = (tfidf_weight * tfidf_sim) + (semantic_weight * semantic_sim)\n",
    "    \n",
    "    # Get top results\n",
    "    top_indices = np.argsort(combined_sim)[-n*3:][::-1]  # Get extra results for filtering\n",
    "    top_matches = []\n",
    "    for idx in top_indices:\n",
    "        if combined_sim[idx] > threshold:\n",
    "            match = {\n",
    "                'question': cancer_df.iloc[idx]['question'],\n",
    "                'answer': cancer_df.iloc[idx]['answer_display'],\n",
    "                'similarity': float(combined_sim[idx]),\n",
    "                'type': 'TF-IDF' if tfidf_sim[idx] > semantic_sim[idx] else 'Semantic',\n",
    "                'tfidf_sim': float(tfidf_sim[idx]),       # <-- ADD THIS LINE\n",
    "                'semantic_sim': float(semantic_sim[idx]) # <-- AND THIS LINE\n",
    "            }\n",
    "            \n",
    "            # Apply disease filter if provided\n",
    "            if not disease_filter or disease_filter.lower() in match['question'].lower():\n",
    "                top_matches.append(match)\n",
    "                \n",
    "            if len(top_matches) >= n:  # Stop when we have enough filtered results\n",
    "                break\n",
    "    if not top_matches:\n",
    "        return [{\n",
    "            'question': \"No confident match found\", # Added\n",
    "            'answer': \"I couldn't find a confident match. Please consult a doctor or rephrase your question.\",\n",
    "            'similarity': 0.0,\n",
    "            'type': 'N/A', # Added\n",
    "            'tfidf_sim': 0.0, # Added\n",
    "            'semantic_sim': 0.0 # Added\n",
    "        }]\n",
    "    \n",
    "    return top_matches[:n]  # Return exactly n results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation\n",
    "\n",
    "To quantitatively assess the effectiveness of the hybrid search system, a dedicated evaluation framework is implemented. This framework uses a `validation_set` containing predefined queries, expected keywords, and specific testing parameters like `disease` filters and `min_similarity` thresholds.\n",
    "\n",
    "The `evaluate_model` function systematically runs each test case from the `validation_set` through the `hybrid_search` function. It then analyzes the retrieved answers to determine if any of the `expected` keywords are present. This provides an indication of whether the system successfully retrieves relevant information.\n",
    "\n",
    "The evaluation process includes:\n",
    "* **Structured Test Cases**: A `validation_data` list defines specific queries with associated `expected` keywords that should appear in a relevant answer, an optional `disease` filter, and a `min_similarity` threshold for that query.\n",
    "* **Hybrid Search Execution**: Each query is passed to the `hybrid_search` function, utilizing the `disease` filter and `min_similarity` for the test.\n",
    "* **Keyword Presence Check**: The function checks if any of the `expected` keywords are present in the `answer` of the top retrieved results.\n",
    "* **Pass/Fail Determination**: A test case is marked as \"PASS\" if at least one expected keyword is found in any of the retrieved answers, indicating a relevant match.\n",
    "* **Detailed Logging**: Comprehensive print statements provide immediate feedback on each test case, including found keywords, best match details, and the overall result (PASS/FAIL).\n",
    "* **Summary Statistics**: A final report provides the total number of tests, passed tests, failed tests, and an overall accuracy score.\n",
    "* **Failed Case Details**: An optional section displays more detailed information for any test cases that failed.\n",
    "\n",
    "This evaluation approach allows for targeted assessment of the model's ability to retrieve information for specific types of cancer-related queries and helps in identifying areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Enhanced validation set with disease-specific testing\n",
    "validation_data = [\n",
    "    {\n",
    "        \"query\": \"prostate cancer symptoms\", \n",
    "        \"expected\": [\"urinary\", \"urination\", \"flow\", \"frequent\", \"weak stream\"],\n",
    "        \"disease\": \"prostate\",\n",
    "        \"min_similarity\": 0.5\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"breast cancer treatment options\", \n",
    "        \"expected\": [\"surgery\", \"radiation\", \"chemotherapy\", \"mastectomy\", \"lumpectomy\"],\n",
    "        \"disease\": \"breast\",\n",
    "        \"min_similarity\": 0.45\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"leukemia causes\", \n",
    "        \"expected\": [\"genetic\", \"mutation\", \"radiation\", \"chemical\", \"chromosome\"],\n",
    "        \"disease\": \"leukemia\",\n",
    "        \"min_similarity\": 0.4\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are the common symptoms of lung cancer?\",\n",
    "        \"expected\": [\"cough\", \"chest pain\", \"shortness of breath\", \"weight loss\"],\n",
    "        \"disease\": \"lung\",\n",
    "        \"min_similarity\": 0.4\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How is melanoma diagnosed?\",\n",
    "        \"expected\": [\"biopsy\", \"skin exam\", \"imaging\"],\n",
    "        \"disease\": \"melanoma\",\n",
    "        \"min_similarity\": 0.45\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Can smoking cause cancer?\",\n",
    "        \"expected\": [\"risk factor\", \"lung\", \"esophagus\", \"mouth\", \"throat\"],\n",
    "        \"disease\": None, # No specific disease filter for general cancer query\n",
    "        \"min_similarity\": 0.35 # Might expect lower similarity for very broad questions\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is the best way to cook pasta?\", # Negative test case\n",
    "        \"expected\": [], # No expected keywords from cancer dataset\n",
    "        \"disease\": None,\n",
    "        \"min_similarity\": 0.1 # Very low threshold to ensure it doesn't match\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: evaluate_model function definition\n",
    "\n",
    "def evaluate_model(validation_set, n_results=1, tfidf_weight=0.4, semantic_weight=0.6):\n",
    "\n",
    "    total_tests = len(validation_set)\n",
    "    passed_tests_count = 0\n",
    "    detailed_evaluation_results = [] # To store detailed results for programmatic analysis\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"MEDICAL CHATBOT EVALUATION REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i, test in enumerate(validation_set):\n",
    "        query = test[\"query\"]\n",
    "        expected_keywords = [k.lower() for k in test[\"expected\"]] # Ensure expected keywords are lowercase\n",
    "        disease_filter = test.get(\"disease\")\n",
    "        min_similarity_threshold = test.get(\"min_similarity\", 0.35) # Use a reasonable default\n",
    "\n",
    "        print(f\"\\nTEST CASE {i+1}: {query}\")\n",
    "        print(f\"DISEASE FILTER: {disease_filter if disease_filter else 'None'}\")\n",
    "        print(f\"EVALUATION THRESHOLD: {min_similarity_threshold:.2f}\")\n",
    "        print(f\"USING HYBRID WEIGHTS: TF-IDF={tfidf_weight:.2f}, Semantic={semantic_weight:.2f}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        # Call hybrid_search with the specified weights for evaluation\n",
    "        # We set threshold=0.0 in hybrid_search to always get 'n_results'\n",
    "        # and then apply min_similarity_threshold manually for evaluation logic.\n",
    "        results_from_hybrid_search = hybrid_search(\n",
    "            query,\n",
    "            n=n_results,\n",
    "            tfidf_weight=tfidf_weight,\n",
    "            semantic_weight=semantic_weight,\n",
    "            threshold=0.0 # Temporarily set threshold low to get top N results for analysis\n",
    "        )\n",
    "        \n",
    "        # Initialize default values for results\n",
    "        test_passed = False\n",
    "        found_keywords_in_top_match = []\n",
    "        top_match_info = {\n",
    "            \"question\": \"No confident match found or below threshold\", # More specific message\n",
    "            \"answer\": \"N/A\",\n",
    "            \"similarity\": 0.0,\n",
    "            \"type\": \"N/A\",\n",
    "            \"tfidf_sim\": 0.0,\n",
    "            \"semantic_sim\": 0.0\n",
    "        }\n",
    "\n",
    "        if results_from_hybrid_search:\n",
    "            # The top match is the first one, as hybrid_search sorts by similarity\n",
    "            current_top_match = results_from_hybrid_search[0]\n",
    "            \n",
    "            top_match_info['question'] = current_top_match['question']\n",
    "            top_match_info['answer'] = current_top_match['answer']\n",
    "            top_match_info['similarity'] = current_top_match['similarity']\n",
    "            top_match_info['type'] = current_top_match['type']\n",
    "            top_match_info['tfidf_sim'] = current_top_match.get('tfidf_sim', 0.0) # Use .get() with default\n",
    "            top_match_info['semantic_sim'] = current_top_match.get('semantic_sim', 0.0) # Use .get() with default\n",
    "\n",
    "            # Check if the top match meets the min_similarity_threshold for this test case\n",
    "            if top_match_info['similarity'] >= min_similarity_threshold:\n",
    "                # Check for expected keywords in the answer of the top match\n",
    "                answer_text_lower = clean_text(top_match_info['answer']).lower() # Ensure consistent cleaning\n",
    "                for keyword in expected_keywords:\n",
    "                    if keyword in answer_text_lower: \n",
    "                        found_keywords_in_top_match.append(keyword)\n",
    "                \n",
    "                # A test passes if it meets the similarity threshold AND finds AT LEAST ONE expected keyword\n",
    "                # If expected_keywords is empty (e.g., for negative tests), it passes if similarity is low.\n",
    "                if (found_keywords_in_top_match) or (not expected_keywords and top_match_info['similarity'] < min_similarity_threshold + 0.1): \n",
    "                    test_passed = True\n",
    "            elif not expected_keywords and top_match_info['similarity'] < min_similarity_threshold:\n",
    "                # Special case: If no keywords are expected (negative test) AND similarity is low, it passes\n",
    "                test_passed = True\n",
    "\n",
    "\n",
    "        print(f\"EXPECTED KEYWORDS: {', '.join(expected_keywords) if expected_keywords else 'None'}\")\n",
    "        print(f\"FOUND KEYWORDS (in top match): {', '.join(found_keywords_in_top_match) if found_keywords_in_top_match else 'None'}\")\n",
    "        print(f\"TOP MATCH SIMILARITY: {top_match_info['similarity']:.2f}\")\n",
    "        print(f\"TOP MATCH TYPE: {top_match_info['type']}\")\n",
    "        \n",
    "        # Print individual similarities if available\n",
    "        if top_match_info['tfidf_sim'] != 0.0 or top_match_info['semantic_sim'] != 0.0:\n",
    "            print(f\"  (TF-IDF Sim: {top_match_info['tfidf_sim']:.2f}, Semantic Sim: {top_match_info['semantic_sim']:.2f})\")\n",
    "\n",
    "        print(f\"BEST MATCH QUESTION: {top_match_info['question'][:100]}...\") # Truncate for display\n",
    "        print(f\"BEST MATCH ANSWER: {top_match_info['answer'][:200]}...\") # Truncate for display\n",
    "        \n",
    "        result_status = \"PASS\" if test_passed else \"FAIL\"\n",
    "        print(f\"RESULT: {result_status}\")\n",
    "\n",
    "        if test_passed:\n",
    "            passed_tests_count += 1\n",
    "        \n",
    "        # Store comprehensive results for each test case\n",
    "        detailed_evaluation_results.append({\n",
    "            \"query\": query,\n",
    "            \"passed\": test_passed,\n",
    "            \"expected_keywords\": expected_keywords,\n",
    "            \"found_keywords\": found_keywords_in_top_match,\n",
    "            \"top_match_info\": top_match_info,\n",
    "            \"evaluation_threshold\": min_similarity_threshold,\n",
    "            \"used_tfidf_weight\": tfidf_weight,\n",
    "            \"used_semantic_weight\": semantic_weight\n",
    "        })\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"TOTAL TESTS: {total_tests}\")\n",
    "    print(f\"PASSED: {passed_tests_count}\")\n",
    "    print(f\"FAILED: {total_tests - passed_tests_count}\")\n",
    "    accuracy = (passed_tests_count / total_tests) * 100 if total_tests > 0 else 0\n",
    "    print(f\"ACCURACY: {accuracy:.2f}%\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(\"\\nFAILED CASE DETAILS:\")\n",
    "    failed_cases_found = False\n",
    "    for res in detailed_evaluation_results:\n",
    "        if not res['passed']:\n",
    "            failed_cases_found = True\n",
    "            print(f\"\\nQuery: {res['query']}\")\n",
    "            print(f\"  Expected Keywords: {', '.join(res['expected_keywords'])}\")\n",
    "            print(f\"  Found Keywords: {', '.join(res['found_keywords']) if res['found_keywords'] else 'None'}\")\n",
    "            print(f\"  Top Match Similarity: {res['top_match_info']['similarity']:.2f} (Threshold: {res['evaluation_threshold']:.2f})\")\n",
    "            print(f\"  Closest Match Question: {res['top_match_info']['question']}\")\n",
    "            print(f\"  Match Answer (partial): {res['top_match_info']['answer'][:200]}...\")\n",
    "            \n",
    "            reason_for_failure = []\n",
    "            if res['top_match_info']['similarity'] < res['evaluation_threshold']:\n",
    "                reason_for_failure.append(\"Similarity below evaluation threshold\")\n",
    "            if res['expected_keywords'] and not res['found_keywords']:\n",
    "                reason_for_failure.append(\"Missing expected keywords\")\n",
    "            # If it's a negative test, and it still got a high score\n",
    "            if not res['expected_keywords'] and res['top_match_info']['similarity'] >= res['evaluation_threshold']:\n",
    "                reason_for_failure.append(\"Unexpected high similarity for negative test\")\n",
    "\n",
    "            print(f\"  Reason for Failure: {'; '.join(reason_for_failure) if reason_for_failure else 'Unknown'}\")\n",
    "    if not failed_cases_found:\n",
    "        print(\"None\")\n",
    "        \n",
    "    return detailed_evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Run the evaluation\n",
    "# This will call the evaluate_model function and automatically print the summary and failed cases.\n",
    "evaluation_results = evaluate_model(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for Model Initialization\n",
    "# This initializes your TF-IDF vectorizer and BioBERT model embeddings.\n",
    "# Make sure 'clean_text' function and 'cancer_df' are already defined and loaded.\n",
    "\n",
    "print(\"Initializing TF-IDF Vectorizer...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(preprocessor=clean_text, stop_words=list(MEDICAL_STOPWORDS))\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(cancer_df['question'])\n",
    "print(f\"TF-IDF Vectorizer fitted. Shape of X_tfidf: {X_tfidf.shape}\")\n",
    "\n",
    "print(\"\\nInitializing BioBERT Embedding Model (this may take a moment)...\")\n",
    "# Ensure 'SentenceTransformer' is imported from 'sentence_transformers'\n",
    "# If you get a ModuleNotFoundError, ensure you've installed it: pip install sentence-transformers\n",
    "model = SentenceTransformer(\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")\n",
    "question_embeddings = model.encode(cancer_df['question'].tolist())\n",
    "print(f\"BioBERT model loaded. Shape of question_embeddings: {question_embeddings.shape}\")\n",
    "print(\"TF-IDF and BioBERT models initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Tuning Experiment\n",
    "\n",
    "This section systematically tunes the contribution of TF-IDF (lexical similarity) and BioBERT embeddings (semantic similarity) within our `hybrid_search` function. The goal is to find the optimal combination of weights (`tfidf_weight` and `semantic_weight`) that maximizes the chatbot's accuracy on the predefined `validation_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for Weight Tuning\n",
    "\n",
    "# Define a range of weights to test. They should sum to 1.0.\n",
    "# np.arange(0.0, 1.05, 0.1) creates values like 0.0, 0.1, 0.2, ..., 1.0\n",
    "weight_combinations = [(round(w, 2), round(1 - w, 2)) for w in np.arange(0.0, 1.05, 0.1)]\n",
    "\n",
    "best_accuracy = -1\n",
    "best_weights = (0.0, 0.0)\n",
    "best_evaluation_results = None # To store detailed results of the best run\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STARTING WEIGHT TUNING EXPERIMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for tfidf_w, semantic_w in weight_combinations:\n",
    "    print(f\"\\n--- Testing Weights: TF-IDF={tfidf_w:.2f}, Semantic={semantic_w:.2f} ---\")\n",
    "    \n",
    "    # Call the evaluate_model function with the current weights\n",
    "    # The evaluate_model will print its own detailed report for each run\n",
    "    current_evaluation_results = evaluate_model(\n",
    "        validation_data,\n",
    "        tfidf_weight=tfidf_w,\n",
    "        semantic_weight=semantic_w\n",
    "    )\n",
    "    \n",
    "    # Calculate accuracy from the returned results to find the best combination\n",
    "    # The 'passed' key in each result dictionary indicates if that test case passed.\n",
    "    current_passed_count = sum(1 for res in current_evaluation_results if res['passed'])\n",
    "    current_accuracy = (current_passed_count / len(validation_data)) * 100 if len(validation_data) > 0 else 0\n",
    "\n",
    "    print(f\"\\nOverall Accuracy for TF-IDF={tfidf_w:.2f}, Semantic={semantic_w:.2f}: {current_accuracy:.2f}%\")\n",
    "\n",
    "    if current_accuracy > best_accuracy:\n",
    "        best_accuracy = current_accuracy\n",
    "        best_weights = (tfidf_w, semantic_w)\n",
    "        best_evaluation_results = current_evaluation_results # Store the full results for detailed analysis later\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"WEIGHT TUNING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"BEST OVERALL ACCURACY: {best_accuracy:.2f}%\")\n",
    "print(f\"OPTIMAL WEIGHTS FOUND: TF-IDF={best_weights[0]:.2f}, Semantic={best_weights[1]:.2f}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Optional: Display FAILED cases from the BEST performing weight combination\n",
    "print(\"\\nDETAILED FAILED CASES FOR OPTIMAL WEIGHTS:\")\n",
    "failed_cases_found_optimal = False\n",
    "if best_evaluation_results:\n",
    "    for res in best_evaluation_results:\n",
    "        if not res['passed']:\n",
    "            failed_cases_found_optimal = True\n",
    "            print(f\"\\nQuery: {res['query']}\")\n",
    "            print(f\"  Expected Keywords: {', '.join(res['expected_keywords']) if res['expected_keywords'] else 'None'}\")\n",
    "            print(f\"  Found Keywords: {', '.join(res['found_keywords']) if res['found_keywords'] else 'None'}\")\n",
    "            print(f\"  Top Match Similarity: {res['top_match_info']['similarity']:.2f} (Threshold: {res['evaluation_threshold']:.2f})\")\n",
    "            print(f\"  Closest Match Question: {res['top_match_info']['question']}\")\n",
    "            print(f\"  Match Answer (partial): {res['top_match_info']['answer'][:200]}...\")\n",
    "            \n",
    "            reason_for_failure = []\n",
    "            if res['top_match_info']['similarity'] < res['evaluation_threshold']:\n",
    "                reason_for_failure.append(\"Similarity below evaluation threshold\")\n",
    "            if res['expected_keywords'] and not res['found_keywords']:\n",
    "                reason_for_failure.append(\"Missing expected keywords\")\n",
    "            if not res['expected_keywords'] and res['top_match_info']['similarity'] >= res['evaluation_threshold']:\n",
    "                reason_for_failure.append(\"Unexpected high similarity for negative test\")\n",
    "            print(f\"  Reason for Failure: {'; '.join(reason_for_failure) if reason_for_failure else 'Unknown'}\")\n",
    "if not failed_cases_found_optimal:\n",
    "    print(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Search Internal Threshold Tuning\n",
    "\n",
    "This section focuses on optimizing the `threshold` parameter within the `hybrid_search` function itself. This threshold determines the minimum combined similarity score a match must achieve to be returned by `hybrid_search`. Finding the optimal threshold is crucial for filtering out irrelevant results (especially for negative test cases) while retaining all highly relevant ones. This tuning is performed using the `best_weights` identified in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for Hybrid Search Internal Threshold Tuning\n",
    "\n",
    "threshold_range = np.arange(0.0, 1.05, 0.05) # Test thresholds from 0.0 to 1.0 (inclusive)\n",
    "best_threshold_for_hybrid_search = 0.0\n",
    "highest_accuracy_for_threshold = -1.0\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STARTING HYBRID SEARCH INTERNAL THRESHOLD TUNING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use the best weights found from the previous tuning step\n",
    "# If `best_weights` is not defined (e.g., if you run this cell alone),\n",
    "# uncomment and set these manually based on your weight tuning results.\n",
    "current_tfidf_weight, current_semantic_weight = best_weights \n",
    "print(f\"Using optimal weights for threshold tuning: TF-IDF={current_tfidf_weight:.2f}, Semantic={current_semantic_weight:.2f}\")\n",
    "\n",
    "threshold_accuracies_for_hybrid_search = {}\n",
    "\n",
    "for threshold_val in threshold_range:\n",
    "    current_passed_count_threshold = 0\n",
    "    current_total_tests_threshold = len(validation_data)\n",
    "    \n",
    "    for test_case in validation_data:\n",
    "        # Call hybrid_search with the current threshold_val and the optimal weights\n",
    "        # Here, `n=1` is typically sufficient as evaluate_model usually only considers the top match.\n",
    "        temp_results = hybrid_search(\n",
    "            test_case['query'],\n",
    "            n=1,\n",
    "            tfidf_weight=current_tfidf_weight,\n",
    "            semantic_weight=current_semantic_weight,\n",
    "            threshold=threshold_val, # THIS IS WHERE THE `hybrid_search`'s internal threshold is set\n",
    "            disease_filter=test_case.get('disease')\n",
    "        )\n",
    "\n",
    "        # Apply the evaluation logic from evaluate_model based on the results from hybrid_search\n",
    "        is_passed_threshold = False\n",
    "        if temp_results: # If hybrid_search returned any result above its internal threshold\n",
    "            top_match = temp_results[0]\n",
    "            # Check against the test case's specific min_similarity_threshold\n",
    "            if top_match['similarity'] >= test_case['min_similarity']:\n",
    "                answer_text_lower = clean_text(top_match['answer']).lower()\n",
    "                found_keywords = [k for k in test_case['expected'] if k.lower() in answer_text_lower]\n",
    "                if found_keywords or (not test_case['expected'] and top_match['similarity'] < test_case['min_similarity'] + 0.1):\n",
    "                    is_passed_threshold = True\n",
    "            # Special case for negative tests: if no keywords expected and low similarity, it passes\n",
    "            elif not test_case['expected'] and top_match['similarity'] < test_case['min_similarity']:\n",
    "                 is_passed_threshold = True\n",
    "        else: # If hybrid_search returned *no results* (because all were below `threshold_val`)\n",
    "            # For negative test cases where no result is expected, if hybrid_search returns nothing, it's a pass.\n",
    "            if not test_case['expected']:\n",
    "                is_passed_threshold = True\n",
    "        \n",
    "        if is_passed_threshold:\n",
    "            current_passed_count_threshold += 1\n",
    "\n",
    "    current_accuracy_threshold = (current_passed_count_threshold / current_total_tests_threshold) * 100\n",
    "    threshold_accuracies_for_hybrid_search[f\"Threshold {threshold_val:.2f}\"] = current_accuracy_threshold\n",
    "    \n",
    "    if current_accuracy_threshold > highest_accuracy_for_threshold:\n",
    "        highest_accuracy_for_threshold = current_accuracy_threshold\n",
    "        best_threshold_for_hybrid_search = threshold_val\n",
    "\n",
    "    print(f\"  Accuracy for Threshold {threshold_val:.2f}: {current_accuracy_threshold:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"HYBRID SEARCH INTERNAL THRESHOLD TUNING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"BEST ACCURACY: {highest_accuracy_for_threshold:.2f}%\")\n",
    "print(f\"OPTIMAL HYBRID SEARCH INTERNAL THRESHOLD: {best_threshold_for_hybrid_search:.2f}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# After identifying the optimal internal threshold, you would usually update the\n",
    "# `threshold` parameter's default value in your `hybrid_search` function definition.\n",
    "# e.g., def hybrid_search(query, n=3, tfidf_weight=0.4, semantic_weight=0.6, threshold=YOUR_OPTIMAL_THRESHOLD_HERE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model's accuracy on a validation set\n",
    "\n",
    "correct = 0\n",
    "total = len(validation_data)\n",
    "k = 3  # Top-k\n",
    "\n",
    "for item in validation_data:\n",
    "    query = item[\"query\"]\n",
    "    expected_keywords = item[\"expected\"]\n",
    "\n",
    "    top_results = hybrid_search(query, n=k, threshold=0.0)\n",
    "\n",
    "    found = any(\n",
    "        any(keyword.lower() in result[\"answer\"].lower() for keyword in expected_keywords)\n",
    "        for result in top_results\n",
    "    )\n",
    "\n",
    "    if found:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Top-{k} Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get a response based on user input with a similarity threshold\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def get_response(user_input, threshold=0.7):\n",
    "    processed_input = clean_text(user_input)\n",
    "    for index, row in cancer_df.iterrows():\n",
    "        question = clean_text(row['question'])\n",
    "        similarity = SequenceMatcher(None, processed_input, question).ratio()\n",
    "        if similarity >= threshold:\n",
    "            return row['answer']\n",
    "    return \"I'm sorry, I don't have information on that. Please consult a doctor.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in hybrid_search(\"Signs of throat cancer?\"):\n",
    "    print(item[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Interface\n",
    "\n",
    "This section sets up an interactive web interface using Gradio, allowing users to query the optimized medical chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def chatbot(query):\n",
    "    results = hybrid_search(query)\n",
    "    top_result = results[0]\n",
    "    if top_result[\"question\"] is None:\n",
    "        return top_result[\"answer\"]\n",
    "    return f\"*Answer: {top_result['answer']}\\nSimilarity*: {top_result['similarity']:.2f}\"\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=chatbot,\n",
    "    inputs=gr.Textbox(label=\"Enter your question about cancer\"),\n",
    "    outputs=gr.Textbox(label=\"Answer\"),\n",
    "    title=\"Cancer QA Chatbot\",\n",
    "    description=\"Ask questions about cancer symptoms, treatments, or risk factors.\"\n",
    ")\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through dedicated effort and a systematic approach, we have successfully developed and optimized a robust medical chatbot designed for cancer information retrieval. Our system stands on a hybrid search methodology, intelligently combining the strengths of traditional TF-IDF with the advanced semantic understanding of BioBERT embeddings.\n",
    "\n",
    "## Challenges Encountered\n",
    "\n",
    "Throughout the development of our medical chatbot for cancer information retrieval, we encountered several key challenges. Overcoming these obstacles was crucial for the successful completion and optimization of the system.\n",
    "\n",
    "1.  **Data Quality and Preprocessing:**\n",
    "    * **Heterogeneous Data Formats:** The initial MedQuAD dataset, while rich in content, presented in XML format required careful parsing to extract the relevant question-answer pairs and associated metadata. Ensuring consistent extraction across diverse XML structures was an initial hurdle.\n",
    "    * **Text Cleaning Complexity:** Medical text often contains specific terminology, abbreviations, and formatting that required tailored preprocessing. Striking the right balance with our `clean_text` function was challenging; aggressive cleaning for search (e.g., extensive lemmatization, stopword removal) could lead to unreadable output for display, while insufficient cleaning hindered retrieval accuracy. This was evident when our initial Gradio output was \"jibberish,\" prompting us to implement a dual-cleaning strategy (one for search, one for display). \n",
    "    * **Duplicate and Near-Duplicate Entries:** Identifying and handling duplicate or near-duplicate question-answer pairs was essential to prevent bias in our index and ensure unique, meaningful responses.\n",
    "\n",
    "2.  **Model Selection and Integration:**\n",
    "    * **Hybrid Search Complexity:** Combining two distinct retrieval methodologies—TF-IDF for lexical matching and BioBERT for semantic understanding—required careful design. Determining how to effectively blend their scores to leverage the strengths of both was a core challenge.\n",
    "    * **Computational Resources for Embeddings:** Encoding the entire dataset's questions into BioBERT embeddings was a computationally intensive task, especially in environments with limited GPU access. This necessitated efficient batch processing and careful memory management.\n",
    "\n",
    "3.  **Performance Tuning and Evaluation:**\n",
    "    * **Defining \"Success\":** Establishing clear and measurable criteria for what constitutes a \"successful\" retrieval was more complex than a simple exact match. Our `evaluate_model` function needed to account for similarity thresholds *and* the presence of expected keywords, particularly for distinguishing relevant from irrelevant answers in negative test cases.\n",
    "    * **Parameter Optimization:** Systematically tuning the `tfidf_weight`, `semantic_weight`, and the internal `threshold` of the `hybrid_search` function was an iterative process. Finding the optimal combination that maximized accuracy across diverse test cases (including negative ones) required careful experimentation and analysis of the evaluation results.\n",
    "    * **Handling Edge Cases/Negative Tests:** The \"What is the best way to cook pasta?\" query highlighted a specific challenge: ensuring genuinely irrelevant queries resulted in either no match or a similarity score below the acceptable threshold. Our initial setup struggled with this, as even semantically unrelated texts could show minor (but still problematic) similarity, leading to \"Unexpected high similarity for negative test\" failures.\n",
    "4.  **User Interface Development:**\n",
    "    * **Seamless Integration:** Connecting the backend `hybrid_search` logic to a frontend interface (Gradio) required careful attention to function signatures and data flow. Ensuring that the output from the retrieval system was correctly formatted and displayed in a user-friendly manner was essential for a good user experience.\n",
    "    * **Displaying Clean vs. Raw Text:** The conflict between text optimized for retrieval (heavily cleaned) and text suitable for human readability (lighter cleaning) for display purposes was a practical challenge that we addressed by maintaining separate `question_cleaned_for_search` and `answer_display` columns. \n",
    "    \n",
    "By systematically addressing these challenges, we were able to refine our approach, improve our code, and ultimately deliver a robust and effective medical chatbot.\n",
    "\n",
    "## Recommendations for Future Improvements\n",
    "\n",
    "While our current model performs exceptionally well on the validation set, we have identified several areas for future improvements to further enhance its capabilities and prepare it for real-world deployment:\n",
    "\n",
    "1.  **Expand and Diversify Training Data:**\n",
    "    * **Larger Dataset:** We recommend incorporating a larger and more diverse dataset. This would cover a broader range of cancer types, related questions, and comprehensive answers, which would significantly improve the model's generalization.\n",
    "    * **Data Annotation:** We believe that manually annotating more data with specific entities (e.g., drug names, symptoms) or intent types would enable us to explore more advanced NLP techniques.\n",
    "    * **Negative Sampling:** Actively curating more diverse negative examples, beyond simple irrelevant queries (like \"cook pasta\"), to include questions somewhat related to the medical domain but not specific to cancer, would greatly improve the model's robustness.\n",
    "\n",
    "2.  **Explore Advanced NLP Models:**\n",
    "    * **Fine-tuning BioBERT/other LLMs:** Instead of solely relying on pre-trained embeddings, fine-tuning BioBERT or other domain-specific Language Models (LLMs) directly on our specific Q&A dataset could further boost performance and understanding of nuances in cancer questions.\n",
    "    * **Generative AI Integration:** For queries where a direct answer isn't immediately found, we could integrate a generative AI model (e.g., a fine-tuned LLM) capable of synthesizing information from retrieved passages to create more conversational and complete answers.\n",
    "\n",
    "3.  **Refine Answer Extraction and Summarization:**\n",
    "    * **Passage Re-ranking:** For the top `n` results retrieved by `hybrid_search`, we could apply a more sophisticated re-ranking model (e.g., a cross-encoder model) to identify the absolute best match, especially when `n` is greater than 1.\n",
    "    * **Answer Span Extraction:** Instead of returning entire answer documents, we aim to implement techniques to extract only the most relevant sentence or paragraph that directly answers the user's query, providing more concise responses.\n",
    "    * **Summarization:** For very long answers, we recommend exploring abstractive or extractive summarization techniques to provide users with a brief, digestible overview."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
